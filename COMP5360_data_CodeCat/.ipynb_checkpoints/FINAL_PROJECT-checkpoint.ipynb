{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc24d93",
   "metadata": {},
   "source": [
    "### Fandom Trends and Pecularities From AO3 Data\n",
    "#### Made by: Rebekah Washburn (u1310114), Noble Ledbetter (u0967666), and Henry Brunisholz (u1276675)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18815ce",
   "metadata": {},
   "source": [
    "### Background and Motivation\n",
    "___\n",
    "\n",
    "The world of fanfiction has been thriving for years. Many popular authors started in fanfiction, and famous novels like *Fifty Shades of Grey* by E. L. James and *After* by Anna Todd started as fan works. As the world of fanfiction and fandom has become more mainstream, sites like [Archive of Our Own (AO3)](https://archiveofourown.org) have held millions of works available for people to enjoy. Both Henry and Rebekah are interested in fanfiction and the data surround these works and are excited to analyze data AO3 specifically. While Noble, an aspiring author, is interested to see trends in the fanfiction universe. We hope to create something meaningful, like Toastystats' fandom statistics, found [here](https://archiveofourown.org/users/destinationtoast/pseuds/toastystats/works?fandom_id=87791).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687044e1",
   "metadata": {},
   "source": [
    "## Project Objectives\n",
    "___\n",
    "We have successfully completed many of the objectives as presented in our project proposal draft, discovering:\n",
    "- Rating does relate to popularity.\n",
    "- The various measures of popularity are tightly correlated.\n",
    "- Length has some relation to popularity.\n",
    "- There are possible mismatches between supply and demand - for example, longer works are broadly speaking more popular but works tend to be short.\n",
    "\n",
    "On the skills side, we got practice with:\n",
    "- cleaning and converting SQLLite to csv.\n",
    "- Cutting up a large data-set into managable chunks.\n",
    "- Sorting through and analyzing a new data-set, from \"the wild\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b9c19",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Fanfiction is the term used to describe written works made by fans about and taking place in media properties - like books and movies. It has been around for a long time, longer if you count derivative works made before the advent of copyright like Virgil's Aeneid - an epic poem in the tradition of Homer's works and with many shared characters and mythical elements.\n",
    "\n",
    "In the modern era, fanfiction is mostly released on the internet on fanfiction hosting websites like fanfiction.net, wattpad, and ArchiveOfOurOwn (AO3).\n",
    "\n",
    "AO3 specifically is unique in that it is an **archive** meaning that the non-profit that made the site explicitly intends it to be used for preservational and even historical purposes, keeping the creative effort of fans around for years to come. In pursuing this purpose, the archive is known to have quite relaxed rules surrounding content, letting many different types of work not allowed on other sites in.\n",
    "\n",
    "The structure of AO3 is quite different from the more commercialized websites, like Wattpad, it lacks any algorthimically generated reccomendations. Instead, users search for works using a highly detailed \"tagging\" system where every fanfiction has tags for fandoms, characters, relationships, and much more. This data is attached to each work, making the tags very useful for a project like this.\n",
    "\n",
    "Finally, each work in the archive has several bits of meta-data like a title, length, and author name. In addition, there is a hit-tracker that counts how many seperate devices/IPs have viewed each work, an option for readers to like the work (called \"kudos\"), an option for readers to comment on each chapter of a work, and an option for readers to save or \"bookmark\" a work, to receive email notifications when the work updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdc9d3b",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "The data we are analyzing was collected in 2020 by reddit user theCodeCat, who scraped non-user-restricted fan-works from ArchiveOfOurOwn (AO3). The data is available for download [here](https://www.reddit.com/r/datasets/comments/i254cw/archiveofourown_dataset/), and is in a SQLite3 database.\n",
    "\n",
    "After processing, our initial sample consists of ~100,000 fan-works with the following information contained in two files - reduced_project_info and reduced_chapter_text.\n",
    "\n",
    "reduced_project_info contains the following data about these fan-works:\n",
    "- id\n",
    "- title (user entered)\n",
    "- date published (user entered)\n",
    "- language (user entered)\n",
    "- rating (user entered)\n",
    "- completion status (user entered)\n",
    "- number of words\n",
    "- number of hits\n",
    "- number of kudos\n",
    "- number of comments\n",
    "- number of bookmarks\n",
    "- description  (user entered)\n",
    "\n",
    "reduced_chapter_text contains the following data for a smaller subset of works:\n",
    "- id\n",
    "- chapter number\n",
    "- chapter text (user entered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa708ac0",
   "metadata": {},
   "source": [
    "### Acquistion\n",
    "\n",
    "Acquistion was done via downloading the scrapped AO3 data provided by theCodeCat, the link in the \"Data Description\" section contains that data available for download.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298785e",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "The data we aquired was already well organized and cleaned, but since it was stored in an SQLite3 database, we needed to extract the information and organize it into panda dataframes. The following is code from our file, \"SQLite3_to_csv\". It will not run properly, as I cannot include the SQLite3 file due to its size. **A note about the following code - almost all sections of code have their own import statements, as we ran the segments needed at different times, depending on the information we were currently retrieving.** \n",
    "\n",
    "We start by grabbing all the data unrelated to each work's chapter text and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# This code grabs the main, numerical data\n",
    "conn = sqlite3.connect(\"organizedData.sqlite3\")\n",
    "df = pd.DataFrame()\n",
    "\n",
    "query_1 = \"SELECT id, title, date, language, restricted, rating, finished, words, hits, kudos, bookmarks, comments, description FROM StoryHeaders LIMIT 100000\"\n",
    "query_2 = \"SELECT * from Languages\"\n",
    "query_3 = \"SELECT * from Ratings\"\n",
    "query_4 = \"SELECT * from FinishedState\"\n",
    "# Info on LIMIT: https://www.w3schools.com/sql/sql_top.asp\n",
    "\n",
    "pd.read_sql_query(query_3, conn)\n",
    "df = pd.read_sql_query(query_1, conn)\n",
    "lan = pd.read_sql_query(query_2, conn)\n",
    "rat = pd.read_sql_query(query_3, conn)\n",
    "fin = pd.read_sql_query(query_4, conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"done\")\n",
    "Once we turned the main data into a data frame, the data frame \"df\" is left with integers in columns that do not make sense, such as language, age rating, and whether the book is finished. These numbers were given text meanings in the other three data frames created. We next take the text info from \"lan\", \"rat\", and \"fin\", and bring it into \"df\".\n",
    "df[\"LanguageName\"] = None\n",
    "df[\"RatingName\"] = None\n",
    "df[\"FinishedStatus\"] = None\n",
    "\n",
    "# I perpetually chain my indexes, so I used these to help me fix my code: https://stackoverflow.com/questions/32357311/python-pandas-how-to-avoid-chained-assignment\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html\n",
    "\n",
    "for index in df.index.to_list():\n",
    "    lan_df = df[\"language\"]\n",
    "    try:\n",
    "        for iden in lan.index.to_list():\n",
    "            if int(lan_df.iloc[index]) == int(lan.loc[iden, \"id\"]):\n",
    "                df.loc[index, \"LanguageName\"] = lan.loc[iden, \"Language_Name\"]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    rat_df = df[\"rating\"]\n",
    "    try:\n",
    "        for iden in rat.index.to_list():\n",
    "            if int(rat_df.iloc[index]) == int(rat.loc[iden, \"id\"]):\n",
    "                df.loc[index, \"RatingName\"] = rat.loc[iden, \"name\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    fin_df = df[\"finished\"]\n",
    "    try:\n",
    "        for iden in fin.index.to_list():\n",
    "            if int(fin_df.iloc[index]) == int(fin.loc[iden, \"id\"]):\n",
    "                df.loc[index, \"FinishedStatus\"] = fin.loc[iden,\"name\"]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f68595",
   "metadata": {},
   "source": [
    "We then store our data in a csv file. Though the next cell would typically be code, I am marking it as text to make sure it is not accidentally run.\n",
    "#### Code\n",
    "import os\n",
    "\n",
    "*#This bit creates a csv of the Data Frame from the above code*\n",
    "\n",
    "*#DO NOT RUN!*\n",
    "\n",
    "try:\n",
    "    os.remove('reduced_project_info.csv')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df.to_csv('reduced_project_info.csv')\n",
    "print('done')\n",
    "\n",
    "#### End of Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff74259",
   "metadata": {},
   "source": [
    "We also wanted to look at chapter text from our collected works. We grab the first 1000 stored chapters. Note that for this part we chose to grab the first 1000 chapters from the initial table they were contained in, so these chapters are not necessarily coming from the same works analyzed with our \"reduced_project_info.csv\" file. We plan to add to this later so that we only look at chapter text for works we are analyzing in the original data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# This code retrieves text from the first 1000 chapters\n",
    "\n",
    "conn = sqlite3.connect(\"organizedData.sqlite3\")\n",
    "query_chapts = \"SELECT storyId, idx, text FROM Chapters LIMIT 1000\"\n",
    "\n",
    "chapters = pd.read_sql_query(query_chapts, conn)\n",
    "\n",
    "# We want the \"text\" column, as someone has already sorted through it for us (I love theCodeCat!)\n",
    "chapters = chapters.rename(columns={\"idx\": \"chapterNumber\"})\n",
    "print(chapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a0b0d",
   "metadata": {},
   "source": [
    "We then save the file in a csv file, like we did with the \"df\" data frame. Since the code for this looks almost identical to the code provided above, I am not including it so it does not get run, and thus wipes our retrieved data. \n",
    "\n",
    "Our last bit of cleaning focuses on the tags attached to works. These tags may contain the fandom the work is about, characters in the story, or key components in the story. We start by grabbing the works and the tagIds attached to each work. In the entire database, there are nearly 90,000,000 tag uses. We used these tags in multiple ways. 1. We analyzed the We have saved ~4100 tags separately to use in a natural language processing model to see if we can predict the category of tag based on the tag name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923dd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# This code will grab the tags assigned to each story, and will grab the tagId's text meaning, and the type\n",
    "# of tag it is. For now, we are just working with the tags assigned to stories. \n",
    "# (Code for grabbing the tagId's name and type is commented out.)\n",
    "\n",
    "conn = sqlite3.connect(\"organizedData.sqlite3\")\n",
    "#query_tags = \"SELECT id, name, type FROM Tags\"\n",
    "query_story_tags = \"SELECT * FROM TagLinks\"\n",
    "\n",
    "# tags_data = pd.read_sql_query(query_tags, conn)\n",
    "story_tags_data = pd.read_sql_query(query_story_tags, conn)\n",
    "\n",
    "# print(story_tags)\n",
    "print(story_tags_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44747597",
   "metadata": {},
   "source": [
    "We then save the \"story_tags_data\" to a csv. We then look at the story ids for the first 10,000 works in our \"reduced_project_info\" file, and open the file we just created for tag data. We then filter our tag data so it only contains tag usage from the 10,000 selected stories. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "data = pd.read_csv(\"story_tags.csv\")\n",
    "sample_dat = pd.read_csv(\"reduced_project_info.csv\")\n",
    "sample_dat = sample_dat.iloc[:10000]\n",
    "\n",
    "unique_works = []\n",
    "\n",
    "for numb in sample_dat[\"id\"]:\n",
    "    if numb not in unique_works:\n",
    "        unique_works.append(numb)\n",
    "\n",
    "iden_numb = data.index.to_list()\n",
    "\n",
    "tag_data = data.copy(deep = True)\n",
    "tag_data = tag_data.iloc[:100000]\n",
    "tag_dat = tag_data.copy(deep= True)\n",
    "\n",
    "numbs = tag_data.index.to_list()\n",
    "\n",
    "count = 0\n",
    "# This just helps me know the code is still running.\n",
    "\n",
    "for iden in numbs:\n",
    "    if int(tag_data.loc[iden, \"storyId\"]) not in unique_works:\n",
    "        tag_dat.drop([iden], inplace = True)\n",
    "        count += 1\n",
    "        print(count)\n",
    "        \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e885a",
   "metadata": {},
   "source": [
    "We then save resave this as a csv. We then use this new file, \"small_story_tags.csv\", and use it to pull the specific tag names and their type from the SQLite3 database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# Assessing Top Tags\n",
    "data = pd.read_csv(\"small_story_tags.csv\")\n",
    "try:\n",
    "    data= data.drop(columns=[\"Unnamed: 0.1\", \"Unnamed: 0\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "counts = data[\"tagId\"].value_counts()\n",
    "print(\"done\")\n",
    "\n",
    "conn = sqlite3.connect(\"organizedData.sqlite3\")\n",
    "\n",
    "\n",
    "for value in counts.index.to_list():\n",
    "    query_1 = f\"SELECT id, name, type FROM Tags WHERE id = {value}\"\n",
    "    df = pd.read_sql_query(query_1, conn)\n",
    "    for index in data.index.to_list():\n",
    "        if data.loc[index, \"tagId\"] == value:\n",
    "            data.loc[index, \"Tag Name\"] = df.loc[0, \"name\"]\n",
    "            data.loc[index, \"Tag Type\"] = df.loc[0, \"type\"]\n",
    "        \n",
    "conn.close()\n",
    "print(data)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ebd08",
   "metadata": {},
   "source": [
    "## Analysis/Methods/Results\n",
    "\n",
    "Our analysis consists of many smaller tasks, the code for these tasks is presented below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Read the data from the file\n",
    "data = pd.read_csv(\"not_reduced_project_info.csv\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(subset=['words', 'kudos', 'comments', 'bookmarks', 'hits'], inplace=True)\n",
    "\n",
    "# Replace infinite values with NaN and drop rows with NaN values\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.dropna(subset=['words', 'kudos', 'comments', 'bookmarks', 'hits'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ea76b",
   "metadata": {},
   "source": [
    "### How correlated are different notions of popularity?\n",
    "\n",
    "To answer this question, we will create a correlation matrix comparing our four possible measures of popularity - hits, kudos, comments, and bookmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c4104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the Correlation Between Different Measures of Popularity\n",
    "col_keep = [\"hits\", \"kudos\", \"comments\", \"bookmarks\"]\n",
    "\n",
    "corr_data = data.copy(deep=True)\n",
    "\n",
    "for col in corr_data.columns:\n",
    "    if col not in col_keep:\n",
    "        corr_data.drop(columns=[col], inplace=True)\n",
    "\n",
    "corr_data = corr_data.corr()\n",
    "\n",
    "heatmap = plt.pcolor(corr_data, cmap=plt.cm.Blues, vmin=-1, vmax=1)\n",
    "ticks = [x + 0.5 for x in range(0, len(corr_data.columns))]\n",
    "labels = [x for x in corr_data.columns]\n",
    "\n",
    "plt.xticks(ticks=ticks, labels=labels, rotation=90)\n",
    "plt.yticks(ticks=ticks, labels=labels)\n",
    "\n",
    "# plot the legend on the side\n",
    "plt.colorbar(heatmap)\n",
    "\n",
    "# Print the correlation values\n",
    "for i, col1 in enumerate(corr_data.columns):\n",
    "    for j, col2 in enumerate(corr_data.columns):\n",
    "        plt.text(j + 0.5, i + 0.5, '{:.2f}'.format(corr_data.iloc[i, j]),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 fontsize=10, color='white')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81550bdf",
   "metadata": {},
   "source": [
    "### Strongly Correlated\n",
    "\n",
    "Our correlation matrix shows that all four measures of popularity are strongly positively correlated with one another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927ef56",
   "metadata": {},
   "source": [
    "### What languages are fanfics written in?\n",
    "\n",
    "To answer this question we will count the number of works under each language in our data set and calculate the % of the total that they form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6415e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by language and count the number of fanfics in each language\n",
    "lang = data.groupby('LanguageName').count()\n",
    "\n",
    "# Calculate the percentage of total works for each language\n",
    "lang['percentage'] = lang['language'] / len(data) * 100\n",
    "\n",
    "# Print the number of works in each language\n",
    "print(lang['language'])\n",
    "print()\n",
    "\n",
    "# Print the number of unique languages in the dataset\n",
    "print('Number of unique languages in dataset:', len(lang))\n",
    "print()\n",
    "\n",
    "# Print the total number of entries in the dataset\n",
    "print('Number of entries in dataset:', len(data))\n",
    "print()\n",
    "\n",
    "# Print the percentage of total works for each language\n",
    "for i in range(len(lang['language'].index)):\n",
    "    perc = round(lang['language'][i] / len(data), 3) * 100\n",
    "    if perc >= 0.1:\n",
    "        print(lang['language'].index[i], 'makes up', str(perc) + '% of the total data.')\n",
    "    else:\n",
    "        print(lang['language'].index[i], 'makes up less than 0.1% of the total data.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c2ebf",
   "metadata": {},
   "source": [
    "### Mostly English\n",
    "\n",
    "The fanworks in this data set are mostly written in English, which makes up 91.4% of the total. The next most used language is Chinese, which makes up 4.5% of the total. Russian is third, with 1.7% of the total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7277b",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "\n",
    "The summary statistics we will provide are:\n",
    "\n",
    "    number of fanfics in the data set\n",
    "    average word count\n",
    "    % of fics finished\n",
    "    number of fics for each rating\n",
    "    average number of hits\n",
    "    average number of kudos\n",
    "    average number of comments\n",
    "    average number of bookmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32258615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of fanfics in the dataset\n",
    "num_fanfics = len(data)\n",
    "\n",
    "# Average word count\n",
    "avg_word_count = data['words'].mean()\n",
    "\n",
    "# Percentage of fics finished\n",
    "percent_finished = (data['finished'].sum() / num_fanfics) * 100\n",
    "\n",
    "# Number of fics for each rating\n",
    "rating_counts = data['RatingName'].value_counts()\n",
    "\n",
    "# Average number of hits\n",
    "avg_hits = data['hits'].mean()\n",
    "\n",
    "# Average number of kudos\n",
    "avg_kudos = data['kudos'].mean()\n",
    "\n",
    "# Average number of comments\n",
    "avg_comments = data['comments'].mean()\n",
    "\n",
    "# Average number of bookmarks\n",
    "avg_bookmarks = data['bookmarks'].mean()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary Statistics\")\n",
    "print(f\"- Number of fanfics in the dataset: {num_fanfics}\")\n",
    "print(f\"- Average word count: {avg_word_count:.2f}\")\n",
    "print(f\"- Percentage of fics finished: {percent_finished:.2f}%\")\n",
    "print(\"- Number of fics for each rating:\")\n",
    "print(rating_counts)\n",
    "print(f\"- Average number of hits: {avg_hits:.2f}\")\n",
    "print(f\"- Average number of kudos: {avg_kudos:.2f}\")\n",
    "print(f\"- Average number of comments: {avg_comments:.2f}\")\n",
    "print(f\"- Average number of bookmarks: {avg_bookmarks:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892de123",
   "metadata": {},
   "source": [
    "### How long are fanfics?\n",
    "To answer this question, we will create a bar chart that visualizes the number of works in each bucket of length, notably we are using the same bin size we use in the \"Are long fanfics more popular?\" section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb35238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins for word counts\n",
    "bins = [0, 5000, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, 55000, 60000, 65000, 70000, 75000, 80000, 85000, 90000, 95000, 100000, float('inf')]\n",
    "\n",
    "# Create a new column in the DataFrame to represent the bin for each fanfic\n",
    "data['word_bin'] = pd.cut(data['words'], bins=bins, labels=['1-5000', '5001-10000', '10001-15000', '15001-20000', '20001-25000', '25001-30000', '30001-35000', '35001-40000', '40001-45000', '45001-50000', '50001-55000', '55001-60000', '60001-65000', '65001-70000', '70001-75000', '75001-80000', '80001-85000', '85001-90000', '90001-95000', '95001-100000', '100001+'])\n",
    "\n",
    "# Count the number of fanfics in each bin\n",
    "fanfic_counts_by_bin = data['word_bin'].value_counts().sort_index()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "fanfic_counts_by_bin.plot(kind='bar', color='skyblue')\n",
    "plt.title('Number of Fanfics by Word Count Bins')\n",
    "plt.xlabel('Word Count Bins')\n",
    "plt.ylabel('Number of Fanfics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print the number of fanfics in each bin with commas\n",
    "for bin_name, count in fanfic_counts_by_bin.items():\n",
    "    print(f\"{bin_name}: {count:,} fanfics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838aa4a",
   "metadata": {},
   "source": [
    "### Fanworks Are Not Very Long\n",
    "\n",
    "Our collection of fanworks are not very long, with more than 4 million - the vast majority of works - being less than 5,000 words. However, there is a long tail of much longer works, which pulls up the average length - as seen in the Summary Statistics section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b897f",
   "metadata": {},
   "source": [
    "### Are long fanfics more popular?\n",
    "To answer this question we will create bar charts comparing the words in a fanfic to our four canidate measures of the popularity of a fanfic - kudos, comments, bookmarks, and hits. Then we will calculate the correlation coefficient between length and our measures of popularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins for word counts\n",
    "bins = [0, 5000, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, 55000, 60000, 65000, 70000, 75000, 80000, 85000, 90000, 95000, 100000, float('inf')]\n",
    "\n",
    "# Create a new column in the DataFrame to represent the bin for each fanfic\n",
    "data['word_bin'] = pd.cut(data['words'], bins=bins, labels=['1-5000', '5001-10000', '10001-15000', '15001-20000', '20001-25000', '25001-30000', '30001-35000', '35001-40000', '40001-45000', '45001-50000', '50001-55000', '55001-60000', '60001-65000', '65001-70000', '70001-75000', '75001-80000', '80001-85000', '85001-90000', '90001-95000', '95001-100000', '100001+'])\n",
    "\n",
    "# Group the DataFrame by the word bins and calculate the average number of hits for each bin\n",
    "average_hits_by_bin = data.groupby('word_bin')['hits'].mean()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "average_hits_by_bin.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Number of Hits by Word Count Bins')\n",
    "plt.xlabel('Word Count Bins')\n",
    "plt.ylabel('Average Number of Hits')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Group the DataFrame by the word bins and calculate the average number of kudos for each bin\n",
    "average_kudos_by_bin = data.groupby('word_bin')['kudos'].mean()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "average_kudos_by_bin.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Number of Kudos by Word Count Bins')\n",
    "plt.xlabel('Word Count Bins')\n",
    "plt.ylabel('Average Number of Kudos')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Group the DataFrame by the word bins and calculate the average number of comments for each bin\n",
    "average_comments_by_bin = data.groupby('word_bin')['comments'].mean()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "average_comments_by_bin.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Number of Comments by Word Count Bins')\n",
    "plt.xlabel('Word Count Bins')\n",
    "plt.ylabel('Average Number of Comments')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Group the DataFrame by the word bins and calculate the average number of bookmarks for each bin\n",
    "average_bookmarks_by_bin = data.groupby('word_bin')['bookmarks'].mean()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "average_bookmarks_by_bin.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Number of Bookmarks by Word Count Bins')\n",
    "plt.xlabel('Word Count Bins')\n",
    "plt.ylabel('Average Number of Bookmarks')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd53cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Pearson correlation coefficient for kudos\n",
    "corr_kudos, p_value_kudos = pearsonr(data['words'], data['kudos'])\n",
    "print(f\"Pearson correlation coefficient (kudos): {corr_kudos:.2f}\")\n",
    "print(f\"P-value (kudos): {p_value_kudos:.4f}\")\n",
    "\n",
    "# Calculate Pearson correlation coefficient for comments\n",
    "corr_comments, p_value_comments = pearsonr(data['words'], data['comments'])\n",
    "print(f\"Pearson correlation coefficient (comments): {corr_comments:.2f}\")\n",
    "print(f\"P-value (comments): {p_value_comments:.4f}\")\n",
    "\n",
    "# Calculate Pearson correlation coefficient and for bookmarks\n",
    "corr_bookmarks, p_value_bookmarks = pearsonr(data['words'], data['bookmarks'])\n",
    "print(f\"Pearson correlation coefficient (bookmarks): {corr_bookmarks:.2f}\")\n",
    "print(f\"P-value (bookmarks): {p_value_bookmarks:.4f}\")\n",
    "\n",
    "# Calculate Pearson correlation coefficient and for hits\n",
    "corr_hits, p_value_hits = pearsonr(data['words'], data['hits'])\n",
    "print(f\"Pearson correlation coefficient (hits): {corr_hits:.2f}\")\n",
    "print(f\"P-value (hits): {p_value_hits:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d1b0c",
   "metadata": {},
   "source": [
    "### Longer Works Are More Popular\n",
    "\n",
    "For all four measures of popularity, there is a nearly monotonic rise in popularity as length increases. This is especially dramatic for the longest works (those over 100,000 words, equivalent to something like 400 pages).\n",
    "\n",
    "The pearson-correlation values confirm this analysis, since they are all positive, indicating that as length increases so does popularity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea80a45",
   "metadata": {},
   "source": [
    "### Are explicit fanfics more popular?\n",
    "To answer this question we will create bar charts comparing a fanfic's rating to our four canidate measures of popularity - hits, kudos, comments, and bookmarks. We will then report if the differences in average popularity between ratings are statistically significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3dd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by rating and calculate the average popularity metrics for each rating\n",
    "average_popularity_by_rating = data.groupby('RatingName').agg({'hits': 'mean', 'kudos': 'mean', 'comments': 'mean', 'bookmarks': 'mean'})\n",
    "\n",
    "# Plot the bar charts for each popularity metric\n",
    "metrics = ['hits', 'kudos', 'comments', 'bookmarks']\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    average_popularity_by_rating[metric].plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'Average {metric.capitalize()} by Rating')\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel(f'Average {metric.capitalize()}')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-test to compare the average popularity metrics between different ratings\n",
    "for metric in metrics:\n",
    "    print(f\"\\nT-test results for {metric.capitalize()}:\\n\")\n",
    "    for i in range(len(average_popularity_by_rating.index)):\n",
    "        for j in range(i+1, len(average_popularity_by_rating.index)):\n",
    "            rating1 = average_popularity_by_rating.index[i]\n",
    "            rating2 = average_popularity_by_rating.index[j]\n",
    "            avg1 = data[data['RatingName'] == rating1][metric]\n",
    "            avg2 = data[data['RatingName'] == rating2][metric]\n",
    "            t_statistic, p_value = ttest_ind(avg1, avg2)\n",
    "            print(f\"T-test between ratings {rating1} and {rating2}:\")\n",
    "            print(f\"  - T-statistic: {t_statistic:.2f}\")\n",
    "            print(f\"  - P-value: {p_value:.4f}\")\n",
    "            if p_value < 0.05:\n",
    "                print(\"  - Statistically significant difference (p < 0.05)\")\n",
    "            else:\n",
    "                print(\"  - No statistically significant difference (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e350e2",
   "metadata": {},
   "source": [
    "### Explicit Works Are More Popular\n",
    "\n",
    "Our analysis shows that the average popularity of a work increases as the rating increases, with explicit works most popular, followed by mature, then teen, then unrated, and finally general rated fics are the least popular.\n",
    "\n",
    "The t-test confirmed this result, indicating that the differences in popularity between ratings are all highly significant and unlikely due to chance alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e1fe9",
   "metadata": {},
   "source": [
    "### Are finished works more popular?\n",
    "To answer this question we will compare the average popularity of finished v. unfinished fanfictions for each of the four candidate measures of popularity, and report on whether the differences are statistically significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559af913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by \"finished\" column and calculate the average popularity metrics for each group\n",
    "average_popularity_by_finished = data.groupby('finished').agg({'hits': 'mean', 'kudos': 'mean', 'comments': 'mean', 'bookmarks': 'mean'})\n",
    "\n",
    "# Plot bar charts comparing the average popularity of finished vs. unfinished fanfictions for each popularity metric\n",
    "metrics = ['hits', 'kudos', 'comments', 'bookmarks']\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    average_popularity_by_finished[metric].plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'Average {metric.capitalize()} by Finished Status')\n",
    "    plt.xlabel('Finished Status')\n",
    "    plt.ylabel(f'Average {metric.capitalize()}')\n",
    "    plt.xticks([1, 0], ['Finished', 'Unfinished'], rotation=0)  # Customize x-axis labels\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16185f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-test to compare the average popularity metrics between finished and unfinished fanfictions\n",
    "print(\"T-test results:\")\n",
    "for metric in metrics:\n",
    "    avg_finished = data[data['finished'] == 0][metric]\n",
    "    avg_unfinished = data[data['finished'] == 1][metric]\n",
    "    t_statistic, p_value = ttest_ind(avg_finished, avg_unfinished)\n",
    "    print(f\"T-test for {metric.capitalize()}:\")\n",
    "    print(f\"  - T-statistic: {t_statistic:.2f}\")\n",
    "    print(f\"  - P-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"  - Statistically significant difference (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"  - No statistically significant difference (p >= 0.05)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2dad1",
   "metadata": {},
   "source": [
    "### Unfinished Works Are More Popular\n",
    "\n",
    "On all four metrics of popularity, it is unfinished works that are more popular than finished works.\n",
    "\n",
    "The t-tests confirm that this difference is highly statistically significant and unlikely to be due to chance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424fc85",
   "metadata": {},
   "source": [
    "### Are English fanfics more popular?\n",
    "\n",
    "To answer this question, we will create a bar chart comparing fanfics written in English with those written in some other language for all four metrics of popularity, and see if the differences in popularity are statistically significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize languages as English or not English\n",
    "def categorize_language(language):\n",
    "    return 'English' if language == 'English' else 'Not English'\n",
    "\n",
    "# Apply the function to create a new column for language categories\n",
    "data['LanguageCategory'] = data['LanguageName'].apply(categorize_language)\n",
    "\n",
    "# Group the DataFrame by \"LanguageCategory\" column and calculate the average popularity metrics for each group\n",
    "average_popularity_by_language = data.groupby('LanguageCategory').agg({'hits': 'mean', 'kudos': 'mean', 'comments': 'mean', 'bookmarks': 'mean'})\n",
    "\n",
    "# Plot bar charts comparing the average popularity of English vs. not English fanfictions for each popularity metric\n",
    "metrics = ['hits', 'kudos', 'comments', 'bookmarks']\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    average_popularity_by_language[metric].plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'Average {metric.capitalize()} by Language')\n",
    "    plt.xlabel('Language')\n",
    "    plt.ylabel(f'Average {metric.capitalize()}')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d816e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-test to compare the average popularity metrics between English and not English fanfictions\n",
    "print(\"T-test results:\")\n",
    "for metric in metrics:\n",
    "    avg_english = data[data['LanguageCategory'] == 'English'][metric]\n",
    "    avg_not_english = data[data['LanguageCategory'] != 'English'][metric]\n",
    "    t_statistic, p_value = ttest_ind(avg_english, avg_not_english)\n",
    "    print(f\"T-test for {metric.capitalize()}:\")\n",
    "    print(f\"  - T-statistic: {t_statistic:.2f}\")\n",
    "    print(f\"  - P-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"  - Statistically significant difference (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"  - No statistically significant difference (p >= 0.05)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac0230",
   "metadata": {},
   "source": [
    "### English Works Are More Popular\n",
    "\n",
    "The charts show that English works have more hits, kudos, comments, and bookmarks than non-English works, with the differences especially pronounced with kudos, comments, and bookmarks.\n",
    "\n",
    "Our t-tests confirm this analysis, all differences are highly significant and unlikely to be due to chance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c293b26",
   "metadata": {},
   "source": [
    "### How popular are the most popular works?\n",
    "To answer this question, we will calculate what percentage of the total number of hits, kudos, comments, and bookmarks the most popular 1%, 5%, and 10% of fics receive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b66e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total hits, kudos, comments, and bookmarks\n",
    "total_hits = data['hits'].sum()\n",
    "total_kudos = data['kudos'].sum()\n",
    "total_comments = data['comments'].sum()\n",
    "total_bookmarks = data['bookmarks'].sum()\n",
    "\n",
    "# Sort the data by hits, kudos, comments, and bookmarks in descending order\n",
    "sorted_data_hits = data.sort_values(by='hits', ascending=False)\n",
    "sorted_data_kudos = data.sort_values(by='kudos', ascending=False)\n",
    "sorted_data_comments = data.sort_values(by='comments', ascending=False)\n",
    "sorted_data_bookmarks = data.sort_values(by='bookmarks', ascending=False)\n",
    "\n",
    "# Calculate the number of fanfics in the top 1%, 5%, and 10% based on hits, kudos, comments, and bookmarks\n",
    "total_fics = len(data)\n",
    "top_1_percent = int(total_fics * 0.01)\n",
    "top_5_percent = int(total_fics * 0.05)\n",
    "top_10_percent = int(total_fics * 0.10)\n",
    "\n",
    "# Calculate the percentage of total hits, kudos, comments, and bookmarks received by the top 1%, 5%, and 10% of fanfics\n",
    "percent_hits_top_1 = sorted_data_hits.head(top_1_percent)['hits'].sum() / total_hits * 100\n",
    "percent_hits_top_5 = sorted_data_hits.head(top_5_percent)['hits'].sum() / total_hits * 100\n",
    "percent_hits_top_10 = sorted_data_hits.head(top_10_percent)['hits'].sum() / total_hits * 100\n",
    "\n",
    "percent_kudos_top_1 = sorted_data_kudos.head(top_1_percent)['kudos'].sum() / total_kudos * 100\n",
    "percent_kudos_top_5 = sorted_data_kudos.head(top_5_percent)['kudos'].sum() / total_kudos * 100\n",
    "percent_kudos_top_10 = sorted_data_kudos.head(top_10_percent)['kudos'].sum() / total_kudos * 100\n",
    "\n",
    "percent_comments_top_1 = sorted_data_comments.head(top_1_percent)['comments'].sum() / total_comments * 100\n",
    "percent_comments_top_5 = sorted_data_comments.head(top_5_percent)['comments'].sum() / total_comments * 100\n",
    "percent_comments_top_10 = sorted_data_comments.head(top_10_percent)['comments'].sum() / total_comments * 100\n",
    "\n",
    "percent_bookmarks_top_1 = sorted_data_bookmarks.head(top_1_percent)['bookmarks'].sum() / total_bookmarks * 100\n",
    "percent_bookmarks_top_5 = sorted_data_bookmarks.head(top_5_percent)['bookmarks'].sum() / total_bookmarks * 100\n",
    "percent_bookmarks_top_10 = sorted_data_bookmarks.head(top_10_percent)['bookmarks'].sum() / total_bookmarks * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Percentage of total hits received by the top 1%, 5%, and 10% of fanfics:\")\n",
    "print(f\"- Top 1%: {percent_hits_top_1:.2f}%\")\n",
    "print(f\"- Top 5%: {percent_hits_top_5:.2f}%\")\n",
    "print(f\"- Top 10%: {percent_hits_top_10:.2f}%\\n\")\n",
    "\n",
    "print(\"Percentage of total kudos received by the top 1%, 5%, and 10% of fanfics:\")\n",
    "print(f\"- Top 1%: {percent_kudos_top_1:.2f}%\")\n",
    "print(f\"- Top 5%: {percent_kudos_top_5:.2f}%\")\n",
    "print(f\"- Top 10%: {percent_kudos_top_10:.2f}%\\n\")\n",
    "\n",
    "print(\"Percentage of total comments received by the top 1%, 5%, and 10% of fanfics:\")\n",
    "print(f\"- Top 1%: {percent_comments_top_1:.2f}%\")\n",
    "print(f\"- Top 5%: {percent_comments_top_5:.2f}%\")\n",
    "print(f\"- Top 10%: {percent_comments_top_10:.2f}%\\n\")\n",
    "\n",
    "print(\"Percentage of total bookmarks received by the top 1%, 5%, and 10% of fanfics:\")\n",
    "print(f\"- Top 1%: {percent_bookmarks_top_1:.2f}%\")\n",
    "print(f\"- Top 5%: {percent_bookmarks_top_5:.2f}%\")\n",
    "print(f\"- Top 10%: {percent_bookmarks_top_10:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fab38d",
   "metadata": {},
   "source": [
    "### The Distribution of Popularity is Highly Unequal\n",
    "\n",
    "The top 1% most popular works recieved between 20% and 35% of all hits, kudos, comments, and bookmarks. The top 5% most popular recieved between 43% and 60%, while the top 10% recieved between 58% and 72%. \n",
    "\n",
    "This is a highly unequal outcome, especially among comments and bookmarks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e34ba87",
   "metadata": {},
   "source": [
    "### Tag Analysis\n",
    "\n",
    "We are still in the early stages of analyzing the tags on our sample fanfics. Due to processing time, we decided to look at the first 100,000 uses of tags. We filtered out any tags that were not from the first 10,000 works in our file, \"reduced_project_info.csv\". This reduced our sample set of tags down to 4194 tags. We then looked at the number of times each specific tag was used, and found the most popular tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# Assessing Top Tags\n",
    "data = pd.read_csv(\"small_story_tags.csv\")\n",
    "try:\n",
    "    data = data.drop(columns= [\"Unnamed: 0\"])\n",
    "    data= data.drop(columns=[\"Unnamed: 0.1\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Top 40 Tags\n",
    "tag_counts = data[\"Tag Name\"].value_counts()[:40]\n",
    "\n",
    "tag_list = []\n",
    "count_list = []\n",
    "for value in tag_counts.index.to_list():\n",
    "    tag_list.append(value)\n",
    "    count_list.append(tag_counts[value])\n",
    "\n",
    "\n",
    "top_40 = pd.DataFrame({ \"Tag Name\": tag_list,\n",
    "                       \"Total Uses\": count_list\n",
    "})\n",
    "unique_stories = []\n",
    "\n",
    "for number in data[\"storyId\"]:\n",
    "    if number not in unique_stories:\n",
    "        unique_stories.append(number)\n",
    "    \n",
    "print(len(unique_stories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c44063",
   "metadata": {},
   "source": [
    "We see that there are 469 works represented in our 4194 tag sample set. This will help us keep in mind the percentage of works that use each of the most popular tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Graphing our Series\n",
    "plt.bar(top_40.index.to_list(), top_40[\"Total Uses\"])\n",
    "plt.xticks(top_40.index.to_list(), top_40[\"Tag Name\"], rotation = 90)\n",
    "plt.show()\n",
    "\n",
    "# print(top_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b457c72",
   "metadata": {},
   "source": [
    "By printing the top_40 data frame, we can see that 354 of the 469 works used the \"No Archive Warnings Apply Tag\". The second most popular tag was \"M/M\", which denotes that a work has a relationship between two men. The third most popular tag was \"Gen\". So the most popular tags seem to support the conclusion from earlier, that being that the majority of fics in our sample set are rated G for General. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7cf5d2",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Our project successfully completed many of our milestones. We collected, cleaned, and analyzed a large amount of data in a variety of ways and along the way discovered many interesting things about fanfiction.\n",
    "\n",
    "However, we did not succeed in every goal. In our project proposal, we stated our desire to analyze how the number of fanfictions on the site AO3 changed over time, however this analysis produced very strange results – like works being uploaded in 1980, decades before the site was available, so we didn’t include this code in our analysis. One possibility behind this oddity is that AO3 lets users manually select the date uploaded when posting a fan work to the site, so if people manually selected 1980 as their publishing date when uploading, that would explain the oddities in the data.\n",
    "\n",
    "Another thing that we wanted to do was create a sentiment analysis of the text and see if positive or negative sentiment were correlated with popularity, however none of our group members knew much about sentiment analysis and the basic version we covered in class seemed to be too basic for our purposes. So, we elected not to perform this analysis.\n",
    "\n",
    "Our project comes with obvious limitations. First, we analyzed only a portion of the fanfiction available on archiveofourown, and the archive only has a portion of the total quantity of fan works made. Thus, it is possible that our analysis will not generalize to fandom as a whole. That said, we have quite a substantial sample – over 6 million works – and no strong reason to believe that our sample is statistically different from fandom as a whole.\n",
    "\n",
    "A second limitation is the risk of confounding. To illustrate this risk, we found that on average the longer a work is the more popular it is. However, this does not mean that length causes a work to be more popular. It could be that a third thing – perhaps writing quality or enthusiasm on the author’s part for the story – produces both length and popularity. It could also be that as a work gets more popular, the fame motivates the author to write more of it – an example of reverse causation. Similar confounding or reverse causation stories could be told for all of the statistical differences we found. However, since we make no claim that we have identified causation only correlation, our statistical analysis is on firmer ground.\n",
    "\n",
    "A third limitation is our project’s reliance on the web-scraping efforts of CoolCat, the reddit user who collected and publicized the data we use for analysis. If they made systemic errors in data collection, organization, or storage those errors would very likely transfer over to our own analysis. Spot-checking the data for particular works on the archive itself showed that CoolCat’s data is accurate, as far as we can tell, however no systematic evidence of accuracy was gathered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f124ab",
   "metadata": {},
   "source": [
    "## Ethical Data Concerns\n",
    "\n",
    "From the project proposal, our concerns were threefold: \n",
    "- First, that the authors of the fanworks analyzed did not consent to their data being collected.\n",
    "- Second, that personal information could be discovered.\n",
    "- Third, that our analysis could contribute to the poor public perception of fanfiction.\n",
    "\n",
    "After project completion, we have a better perspective on these ethical risks:\n",
    "- Firstly, while authors were not contacted to ask for their consent, the works that were scraped and have usable data were all unrestricted, meaning they were publicly accessible by anyone visiting AO3. This data was also collected and shared publically by the CoolCat; we did not scrape the data ourselves. As mentioned in our proejct proposal, since the release of AI tools such as Chat GPT, some AO3 authors have been upset that machine learning could be used on their works and used to create \"new\" fan works. Our project will never try to take the gathered text and reproduce a similar work.\n",
    "- Secondly, while there may be personal information in the data we are looking at, our data analysis has not uncovered any of this information.\n",
    "- Thirdly, while fanfiction does have a negative reputation in the public eye, we still think that since we are not publishing our analysis in public that our project will have little reputational impact regardless of its findings. That being said, we have discovered that in our sample, more explicit/mature works are more popular among fanfic readers. This reinforces the popular perception of fanfic-as-porn. However, we also found that a majority of works in our sample are rated \"General\" or \"Teen\", implying that most fanfic written is not pornographic, even if more explicit works are more popular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61658f4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Using data from a web-scrapping project, we dowloanded information about millions of fan-works published on Archive Of Our Own. Then, after cleaning and converting the data into csv format, we conducted a variety of statistical tests on the \"meta-data\" of each fan-work, as presented in the **Analysis/Methods/Results** section. We also analyzed the tags associated with each fan-work.\n",
    "\n",
    "From this analysis we have come to several conclusions:\n",
    "- The different notions of popularity - hits, kudos, comments, and bookmarks - are positively correlated.\n",
    "- Fanworks tend to be short, mostly under 5000 words, although there is high variance.\n",
    "- Most works are published in English, with Chinese and Russian distant runners-up.\n",
    "- Longer works tend to be more popular.\n",
    "- Explicit works tend to be more popular.\n",
    "- Unfinished works tend to be more popular.\n",
    "- Works in English tend to be more popular.\n",
    "- The distribution of popularity is highly unequal, with the most popular works recieving a highly disproportionate number of the total hits, kudos, comments, and bookmarks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
