{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9cc1824",
   "metadata": {},
   "source": [
    "# Project Proposal\n",
    "\n",
    "### Basic Info\n",
    "_____\n",
    " \n",
    "**Title**: Fandom Trends and Pecularities From AO3 Data\n",
    " \n",
    "\n",
    "**Names**: Rebekah Washburn, Nobel Ledbetter, Henry Brunisholz\n",
    " \n",
    "\n",
    "**Emails**: Rebekah - u1310114@utah.edu, Nobel - u0967666@utah.edu, Henry - u1276675@utah.edu\n",
    " \n",
    "\n",
    "**UIDs**: Rebekah - u1310114, Nobel - u0967666, Henry - u1276675\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Background and Motivation\n",
    "___\n",
    "\n",
    "Both Rebekah and Henry are fans of fanfiction and data surrounding it, for example both were aware of Toastystats' fandom statistics, found [here](https://archiveofourown.org/users/destinationtoast/pseuds/toastystats/works?fandom_id=87791).\n",
    "### Project Objectives\n",
    "Project objectives can be broken into two main groups:\n",
    "- Questions about the data: How does rating relate to a fics popularity? How closely connected are varying defintions of popularity (comment count, hit count, kudos count)? How does length relate to popularity? Are there mismatches in \"supply and demand\" where a certain kind of fic is very popular among fans but not produced very frequently by writers? What about the other way around, where writers write a lot of not-so-popular stories?\n",
    "- Data Analysis Skill-development: How do we handle a large data set like the one we found? How do you clean and organize data \"from the wild\" so to speak, as opposed to a class example? How best are any findings displayed using charts and other data visualization tools?\n",
    "\n",
    "### Data Description and Acquistion\n",
    "___\n",
    "The data we are analyzing is a dataset collected by reddit user theCodeCat in 2020, who scrapped non-user-restricted fan-works from ArchiveOfOurOwn (AO3). The data is available for download [here](https://www.reddit.com/r/datasets/comments/i254cw/archiveofourown_dataset/).\n",
    "\n",
    "The dataset is 77GB when compressed and 502GB uncompressed, containing data from millions of works on AO3 including:\n",
    "- id\n",
    "- rating\n",
    "- whether it is finished\n",
    "- title\n",
    "- description\n",
    "- current number of chapters\n",
    "- planned number of chapters\n",
    "- language\n",
    "- word count\n",
    "- hit count\n",
    "- comment count (but not the commends themselves)\n",
    "- bookmark count\n",
    "- date published\n",
    "- Authors\n",
    "- Users/Authors work is dedicated to\n",
    "- Series work is a part of if applicable\n",
    "- Tags (warnings, fandoms, relationships, characters, relationship types, generic)\n",
    "- Chapter text\n",
    "\n",
    "### Ethical Considerations\n",
    "___\n",
    "Ethical considerations for the use and analysis of this data are fortunately limited, but not non-existent.\n",
    "\n",
    "First, there is the fact that many fanfic writers do not want their fanfics used for data analysis (or machine learning!) projects, as emphasized by the general uproar on the internet following the revelation that LLM's like ChatGPT were trained on data that included fanfiction published to the internet. Using the dataset collected by theCoolCat mostly avoids this ethical quandry, because theCoolCat was the one who collected the data and distributed it to the world, it won't change any fanfic writer's experience for us to privately analyze that data.\n",
    "\n",
    "Second, there could be personal or upsetting or sensitive information contained in this data set, such as folks' real names or addresses, personal and private experiences, or simply content not suitable for professional environments like graphic sex scenes. This risk is magnified because the dataset contains the chapter *text* of each fanfiction, meaning that the actual words written in each story in the dataset are included in the dataset itself. But, the risk is minimized because we will not be reading the chapter texts ourselves, rather letting the computer do it and using data analytic techniques to summarize the text/meta-data for each piece of fanfiction.\n",
    "### Data Cleaning and Processing\n",
    "### Exploratory Analysis\n",
    "### Analysis Methodology\n",
    "### Project Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac640a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
