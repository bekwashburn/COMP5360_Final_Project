{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9cc1824",
   "metadata": {},
   "source": [
    "# Project Proposal\n",
    "\n",
    "### Basic Info\n",
    "_____\n",
    " \n",
    "**Title**: Fandom Trends and Pecularities From AO3 Data\n",
    " \n",
    "\n",
    "**Names**: Rebekah Washburn, Noble Ledbetter, Henry Brunisholz\n",
    " \n",
    "\n",
    "**Emails**: Rebekah - u1310114@utah.edu, Nobel - u0967666@utah.edu, Henry - u1276675@utah.edu\n",
    " \n",
    "\n",
    "**UIDs**: Rebekah - u1310114, Nobel - u0967666, Henry - u1276675\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Background and Motivation\n",
    "___\n",
    "\n",
    "The world of fanfiction has been thriving for years. Many popular authors started in fanfiction, and some popular novels like *Fifty Shades of Grey* by E. L. James and *After* by Anna Todd. As the world of fanfiction and fandom has become more mainstream, sites like [Archive of Our Own](https://archiveofourown.org) have held millions of works available for people to enjoy. Both Henry and Rebekah are interested in fanfiction and the data surround these works, and previous projects by individuals to analyze said data. We hope to create something meaningful, like Toastystats' fandom statistics, found [here](https://archiveofourown.org/users/destinationtoast/pseuds/toastystats/works?fandom_id=87791).\n",
    "\n",
    "### Project Objectives\n",
    "___\n",
    "Project objectives can be broken into two main groups:\n",
    "- Questions about the data: How does age rating relate to a fics popularity? How closely connected are varying defintions of popularity (comment count, hit count, kudos count)? How does length relate to popularity? Are there mismatches in \"supply and demand\" where a certain kind of fic is very popular among fans but not produced very frequently by writers? What about the other way around, where writers write a lot of not-so-popular stories? How complicated/readable is the average fanfiction? How does that vary with measurements of popularity?\n",
    "- Data Analysis Skill-development: How do we handle a large data set like the one we found? How do you clean and organize data \"from the wild\" so to speak, as opposed to a class example? How best are any findings displayed using charts and other data visualization tools? How do we determine whether something is a useful question to analyze? \n",
    "\n",
    "### Data Description and Acquistion\n",
    "___\n",
    "The data we are analyzing is a dataset collected by reddit user theCodeCat in 2020, who scrapped non-user-restricted fan-works from ArchiveOfOurOwn (AO3). The data is available for download [here](https://www.reddit.com/r/datasets/comments/i254cw/archiveofourown_dataset/), and is in a SQLite database. None of us have yet worked with a SQLite Database, so we will need to explore if there are formatting changes needed to get it ready for Pandas. \n",
    "\n",
    "The dataset is 77GB when compressed and 502GB uncompressed, containing data from millions of works on AO3 including:\n",
    "- id\n",
    "- rating\n",
    "- whether it is finished\n",
    "- title\n",
    "- description\n",
    "- current number of chapters\n",
    "- planned number of chapters\n",
    "- language\n",
    "- word count\n",
    "- hit count\n",
    "- comment count (but not the comments themselves)\n",
    "- bookmark count\n",
    "- date published\n",
    "- Authors\n",
    "- Users/Authors work is dedicated to\n",
    "- Series work is a part of if applicable\n",
    "- Tags (warnings, fandoms, relationships, characters, relationship types, generic)\n",
    "- Chapter text\n",
    "\n",
    "### Ethical Considerations\n",
    "___\n",
    "Ethical considerations for the use and analysis of this data are fortunately limited, but not non-existent.\n",
    "\n",
    "First, there is the fact that many fanfic writers do not want their fanfics used for data analysis (or machine learning!) projects, as emphasized by the general uproar on the internet following the revelation that LLM's like ChatGPT were trained on data that included fanfiction published to the internet. Using the dataset collected by theCoolCat mostly avoids this ethical quandry, because theCoolCat was the one who collected unrestricted data and distributed it to the world, it won't change any fanfic writer's experience for us to privately analyze that data.\n",
    "\n",
    "Second, there could be personal or upsetting or sensitive information contained in this data set, such as folks' real names or addresses, personal and private experiences, or simply content not suitable for professional environments like graphic sex scenes. This risk is magnified because the dataset contains the chapter *text* of each fanfiction, meaning that the actual words written in each story in the dataset are included in the dataset itself. But, the risk is minimized because we will not be reading the chapter texts ourselves, rather letting the computer do it and using data analytic techniques to summarize the text/meta-data for each piece of fanfiction.\n",
    "\n",
    "Third, there is a popular view of fanfiction as derivative, illegal, and pornographic writing of low quality. If our data analysis finds that, for example, fanfic tends to have lower word length/sentence length than books, that could contribute to this public stigma. However, this ethical risk is reduced because our project will be done based off data publically avaialable already, meaning any conclusions we draw from it could have been drawn already. Additionally, it is arguable that this is an ethical risk at all. Something in our data may or may not be able to be interpreted by fandom's detractors in a negative light, but, as the saying goes, haters gonna hate.\n",
    "\n",
    "### Data Cleaning and Processing\n",
    "___\n",
    "We will certainly have to do substantial data extraction and cleanup due to the size of the data we have. Thankfully, similar tags in our data (such as words, kudos, and hits) will allow us to simplify some of this extraction.\n",
    "\n",
    "The quantities we expect to pull from our data are many. First, we are planning on creating correlations matrices between different notions of popularity, like hits, kudos, and comments, to see how tightly they are connected - all those quantities have to be gathered. Second, we want to see how well notions of popularity correlate with other meta-data, like the length of the work, number of chapters of the work, the work's rating, and whether the work is completed or not - all of these quantities must also be collected. Further, we want to do a complexity/readability analysis, where the average word length and sentence length are collated for analysis and potential comparison with other quantities previously collected. We also want to perform a rudimentary \"sentiment analysis\" where the emotional valence of different words is classified as positive and negative, these quantities will be collected and compared against previously collected ones like length or popularity. \n",
    "\n",
    "The ideal scenario will be to do data processing in similar ways as it has been done in class, through PANDAS. As the dataset is stored in an SQLite database, this may not be possible, more research into how to handle sqlite databases is needed.\n",
    "\n",
    "Some challenges we will have to our processing will be sorting through the numerous custom tags (\"freeforms\" in the html). These custom tags are specific to the book and designed by the author, however, since we are focusing more on standardized data (such as word count, genre, and kudos) we will be able to focus on the main data we want and then sort through custom tags as needed. Depending on how the data is organized, we may be able to count the uses of common tags, such as the genre of a work.\n",
    "\n",
    "### Exploratory Analysis\n",
    "___\n",
    "We will use scatterplots and bar charts primarily to determine trends in our data, like how kudos relate to the completeness of the fanfic or if rating increases based on the length of the fanfic. We will also look at correlation heatmaps to see whether different levels of popularity - kudos (likes), bookmarks, hits - have any correlation with one another. If they do not, this could mean that certain fandoms show appreciation or interest for works in different ways. Using the altair or seaborn programs, we can also use a third indicator to visualize other trends within our data. \n",
    "\n",
    "### Analysis Methodology\n",
    "___\n",
    "The specific questions we want to answer are as follows:\n",
    "- How closely connected are the three candidate notions of popularity (hits, kudos, comments)? Is there are sensible way to combine the three metrics to get a more holistic view of a fic's popularity?\n",
    "- How do varying quantitative measurements correlate with fic popularity, like length, chapter count, completition status, rating, complexity/readability, and emotional valence?\n",
    "- Do any of the answers to the previous questions change based on the fandom being considered? \n",
    "- How do large fandoms with many works in the dataset compare to smaller fandoms with fewer works? Are there differences in the mean popularity, length, rating, completition status, chapter counts, readability, or emotional valence?\n",
    "- Based on the above, if one were trying to create the most statistically popular fanfiction, what would it look like?\n",
    "\n",
    "The techniques we plan to use are, tentatively, as follows:\n",
    "- Ordinary Least Squares Regression, to find the strength of the correlation between two of our collected quantities.\n",
    "- Controlling for variables to identify and remove potential confounding variables.\n",
    "- Tests of statistical significance, like the t-test and the z-test, to find if mean differences between quantities are statistically significant or not.\n",
    "- Sentiment analysis, based off of a simple categorization of words into positive, negative, and neutral buckets.\n",
    "- Readability analysis, inspired by popular equations for finding the readability of texts like the Flesch-Kincaid scale, but without counting syllables (unless we find a way for the computer to count syllables).\n",
    "\n",
    "### Project Schedule\n",
    "___\n",
    "\n",
    "The project is due April 19th, while the first milestone is due April 3rd. Our goal is to have a rudimentary version of the project done by the third. This means that the cleaning, data analysis, and visualization components will hopefully be at least started by the third, especially the cleaning portion.\n",
    "\n",
    "- Week 1 (March 17th - March 23th): Data cleaning and sorting, break the large data file into more easily used chunks.\n",
    "- Week 2 (March 24th - March 30st): Complete some analysis tasks, perhaps simple correlations between easily found quantities like notions of popularity, fic length, rating, and completition status.\n",
    "- Week 3 (March 31st - April 6th): Complete more analysis tasks, perhaps the sentiment analysis or readability analysis, turn in the first milestone on the third.\n",
    "- Week 4 (April 7th - April 13th): Finish analysis tasks, begin on data visualization.\n",
    "- Week 5 (April 13th - April 19th): Complete visualization and presentation components of project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac640a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
